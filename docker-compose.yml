version: '3.8'

services:
  # Zookeeper for Kafka
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    hostname: zookeeper
    container_name: zookeeper
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    volumes:
      - zookeeper-data:/var/lib/zookeeper/data
      - zookeeper-logs:/var/lib/zookeeper/log
    networks:
      - streaming-network

  # Kafka Broker
  kafka:
    image: confluentinc/cp-kafka:7.5.0
    hostname: kafka
    container_name: kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
      - "9997:9997"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
      KAFKA_JMX_PORT: 9997
      KAFKA_JMX_HOSTNAME: localhost
    volumes:
      - kafka-data:/var/lib/kafka/data
    networks:
      - streaming-network

  # Schema Registry
  schema-registry:
    image: confluentinc/cp-schema-registry:7.5.0
    hostname: schema-registry
    container_name: schema-registry
    depends_on:
      - kafka
    ports:
      - "8081:8081"
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: 'kafka:29092'
      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081
    networks:
      - streaming-network

  # PostgreSQL Source Database
  postgres:
    image: postgres:15
    hostname: postgres
    container_name: postgres
    ports:
      - "5433:5432"
    environment:
      POSTGRES_DB: sourcedb
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_INITDB_ARGS: "--wal-level=logical"
    command:
      - "postgres"
      - "-c"
      - "wal_level=logical"
      - "-c"
      - "max_connections=200"
      - "-c"
      - "max_replication_slots=4"
      - "-c"
      - "max_wal_senders=4"
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./docker/postgres/init:/docker-entrypoint-initdb.d
    networks:
      - streaming-network

  # Kafka Connect with Debezium
  kafka-connect:
    image: debezium/connect:2.4
    hostname: kafka-connect
    container_name: kafka-connect
    depends_on:
      - kafka
      - postgres
      - schema-registry
    ports:
      - "8083:8083"
    environment:
      BOOTSTRAP_SERVERS: kafka:29092
      GROUP_ID: 1
      CONFIG_STORAGE_TOPIC: my_connect_configs
      CONFIG_STORAGE_REPLICATION_FACTOR: 1
      OFFSET_STORAGE_TOPIC: my_connect_offsets
      OFFSET_STORAGE_REPLICATION_FACTOR: 1
      STATUS_STORAGE_TOPIC: my_connect_statuses
      STATUS_STORAGE_REPLICATION_FACTOR: 1
      KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry:8081
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry:8081
    volumes:
      - ./config/debezium:/kafka/config
    networks:
      - streaming-network

  # LocalStack for AWS services (Kinesis)
  localstack:
    image: localstack/localstack:3.0
    hostname: localstack
    container_name: localstack
    ports:
      - "4566:4566"
      - "4510-4559:4510-4559"
    environment:
      DEBUG: 1
      SERVICES: kinesis,dynamodb,s3,lambda
      DATA_DIR: /tmp/localstack/data
      DOCKER_HOST: unix:///var/run/docker.sock
      HOST_TMP_FOLDER: /tmp/localstack
    volumes:
      - "/tmp/localstack:/tmp/localstack"
      - "/var/run/docker.sock:/var/run/docker.sock"
      - ./docker/localstack/init:/etc/localstack/init/ready.d
    networks:
      - streaming-network

  # Redis for caching and session management
  redis:
    image: redis:7-alpine
    hostname: redis
    container_name: redis
    ports:
      - "6380:6379"
    command: redis-server --appendonly yes
    volumes:
      - redis-data:/data
    networks:
      - streaming-network

  # Kafka UI for monitoring
  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    hostname: kafka-ui
    container_name: kafka-ui
    depends_on:
      - kafka
      - schema-registry
      - kafka-connect
    ports:
      - "8082:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:29092
      KAFKA_CLUSTERS_0_SCHEMAREGISTRY: http://schema-registry:8081
      KAFKA_CLUSTERS_0_KAFKACONNECT_0_NAME: first
      KAFKA_CLUSTERS_0_KAFKACONNECT_0_ADDRESS: http://kafka-connect:8083
    networks:
      - streaming-network

  # Prometheus for metrics
  prometheus:
    image: prom/prometheus:latest
    hostname: prometheus
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./config/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=200h'
      - '--web.enable-lifecycle'
    networks:
      - streaming-network

  # Grafana for visualization
  grafana:
    image: grafana/grafana:latest
    hostname: grafana
    container_name: grafana
    ports:
      - "3000:3000"
    environment:
      GF_SECURITY_ADMIN_USER: admin
      GF_SECURITY_ADMIN_PASSWORD: admin
    volumes:
      - grafana-data:/var/lib/grafana
      - ./config/grafana/provisioning:/etc/grafana/provisioning
    networks:
      - streaming-network

  # FastAPI Streaming Application
  streaming-app:
    build: .
    hostname: streaming-app
    container_name: streaming-app
    depends_on:
      - kafka
      - postgres
      - redis
      - localstack
    ports:
      - "8888:8000"
    environment:
      - DATABASE_URL=postgresql://postgres:postgres@postgres:5432/sourcedb
      - KAFKA_BOOTSTRAP_SERVERS=kafka:29092
      - KAFKA_SERVERS=kafka:29092
      - REDIS_URL=redis://redis:6379
      - KINESIS_ENDPOINT_URL=http://localstack:4566
      - AWS_ACCESS_KEY_ID=test
      - AWS_SECRET_ACCESS_KEY=test
      - AWS_REGION=us-east-1
      - LOG_LEVEL=INFO
      - ENABLE_STREAM_PROCESSING=true
      - ENABLE_CROSS_PLATFORM_STREAMING=true
      - ENABLE_KAFKA_OUTPUT=true
      - ENABLE_KINESIS_OUTPUT=true
    volumes:
      - ./app:/app/app:ro
      - ./logs:/app/logs
    networks:
      - streaming-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Apache Flink JobManager
  flink-jobmanager:
    image: flink:1.18.0
    hostname: flink-jobmanager
    container_name: flink-jobmanager
    ports:
      - "8085:8081"
    command: jobmanager
    environment:
      FLINK_PROPERTIES: |
        jobmanager.rpc.address: flink-jobmanager
        jobmanager.memory.process.size: 1024m
        taskmanager.memory.process.size: 1024m
    volumes:
      - ./flink-jobs:/opt/flink/usrlib
      - ./logs/flink:/opt/flink/log
    networks:
      - streaming-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Apache Flink TaskManager
  flink-taskmanager:
    image: flink:1.18.0
    depends_on:
      - flink-jobmanager
    command: taskmanager
    scale: 2
    environment:
      FLINK_PROPERTIES: |
        jobmanager.rpc.address: flink-jobmanager
        taskmanager.memory.process.size: 1024m
    volumes:
      - ./flink-jobs:/opt/flink/usrlib
      - ./logs/flink:/opt/flink/log
    networks:
      - streaming-network
    restart: unless-stopped

  # Flink SQL Gateway (선택사항)
  flink-sql-gateway:
    image: flink:1.18.0
    hostname: flink-sql-gateway
    container_name: flink-sql-gateway
    depends_on:
      - flink-jobmanager
    ports:
      - "8084:8083"
    command: |
      bash -c "
        /opt/flink/bin/sql-gateway.sh start -Dsql-gateway.endpoint.rest.address=0.0.0.0;
        tail -f /dev/null
      "
    environment:
      FLINK_PROPERTIES: |
        jobmanager.rpc.address: flink-jobmanager
    volumes:
      - ./flink-jobs:/opt/flink/usrlib
    networks:
      - streaming-network
    restart: unless-stopped

volumes:
  zookeeper-data:
  zookeeper-logs:
  kafka-data:
  postgres-data:
  redis-data:
  prometheus-data:
  grafana-data:

networks:
  streaming-network:
    driver: bridge